# GPT4All Node.js API

```sh
yarn install gpt4all@alpha

npm install gpt4all@alpha

pnpm install gpt4all@alpha
```

The original [GPT4All typescript bindings](https://github.com/nomic-ai/gpt4all-ts) are now out of date.

*   New bindings created by [jacoobes](https://github.com/jacoobes) and the [nomic ai community](https://home.nomic.ai) :D, for all to use.
*   [Documentation](#Documentation)
### Code (alpha)

```js
import { createCompletion, loadModel } from '../src/gpt4all.js'

const ll = await loadModel('ggml-vicuna-7b-1.1-q4_2.bin', { verbose: true });

const response = await createCompletion(ll, [
    { role : 'system', content: 'You are meant to be annoying and unhelpful.'  },
    { role : 'user', content: 'What is 1 + 1?'  } 
]);

```

### API

*   The nodejs api has made strides to mirror the python api. It is not 100% mirrored, but many pieces of the api resemble its python counterpart.
*   Everything should work out the box.
*   [docs](./docs/api.md)

### Build Instructions

*   As of 05/21/2023, Tested on windows (MSVC). (somehow got it to work on MSVC ðŸ¤¯)
    *   binding.gyp is compile config
*   Tested on Ubuntu. Everything seems to work fine
*   MingW works as well to build the gpt4all-backend. **HOWEVER**, this package works only with MSVC built dlls.

### Requirements

*   git
*   [node.js >= 18.0.0](https://nodejs.org/en)
*   [yarn](https://yarnpkg.com/)
*   [node-gyp](https://github.com/nodejs/node-gyp)
    *   all of its requirements.
*   (unix) gcc version 12
    *   These bindings use the C++ 20 standard.
*   clang does not seem to work, no work around has been discovered 
    *   Reason being is that the frontend for this library uses a cpp 20 standard construct (shared atomic ptrs)
*   (win) msvc version 143
    *   Can be obtained with visual studio 2022 build tools

### Build (from source)

```sh
git clone https://github.com/nomic-ai/gpt4all.git
cd gpt4all-bindings/typescript
```

*   The below shell commands assume the current working directory is `typescript`.

*   To Build and Rebuild:

Also if you do not have cpp 20 standard on gcc:
Tested on Ubuntu

```sh 
sudo apt install g++-12 gcc-12
sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-12 100 --slave /usr/bin/g++ g++ /usr/bin/g++-12 --slave /usr/bin/gcov gcov /usr/bin/gcov-12
```

```sh
yarn
```

*   llama.cpp git submodule for gpt4all can be possibly absent. If this is the case, make sure to run in llama.cpp parent directory

```sh
git submodule update --init --depth 1 --recursive
```

**AS OF NEW BACKEND** to build the backend,

```sh
yarn build:backend
```

This will build platform-dependent dynamic libraries, and will be located in runtimes/(platform)/native The only current way to use them is to put them in the current working directory of your application. That is, **WHEREVER YOU RUN YOUR NODE APPLICATION**

*   llama-xxxx.dll is required.
*   According to whatever model you are using, you'll need to select the proper model loader.
    *   For example, if you running an Mosaic MPT model, you will need to select the mpt-(buildvariant).(dynamiclibrary)

### Test

```sh
yarn test
```

### Source Overview

#### src/

*   Extra functions to help aid devex
*   Typings for the native node addon
*   the javascript interface

#### test/

*   simple unit testings for some functions exported.
*   more advanced ai testing is not handled

#### spec/

*   Average look and feel of the api
*   Should work assuming a model and libraries are installed locally in working directory

#### index.cc

*   The bridge between nodejs and c. Where the bindings are.

#### prompt.cc

*   Handling prompting and inference of models in a threadsafe, asynchronous way.

#### docs/

*   Autogenerated documentation using the script `yarn docs:build`

### Roadmap

This package is in active development, and breaking changes may happen until the api stabilizes. Here's what's the todo list:

*   \[x] prompt models via a threadsafe function in order to have proper non blocking behavior in nodejs
*   \[ ] createTokenStream, an async iterator that streams each token emitted from the model. Planning on following this [example](https://github.com/nodejs/node-addon-examples/tree/main/threadsafe-async-iterator)
*   \[ ] proper unit testing (integrate with circle ci)
*   \[ ] publish to npm under alpha tag `gpt4all@alpha`
*   \[x] have more people test on other platforms (mac tester needed)
*   \[x] switch to new pluggable backend

### Documentation
