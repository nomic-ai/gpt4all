### Javascript Bindings

The original [GPT4All typescript bindings](https://github.com/nomic-ai/gpt4all-ts) are now out of date.

*   created by [jacoobes](https://github.com/jacoobes) and [nomic ai](https://home.nomic.ai) :D, for all to use.

### Code (alpha)

```js
import { LLModel, createCompletion, DEFAULT_DIRECTORY, DEFAULT_LIBRARIES_DIRECTORY } from '../src/gpt4all.js'

const ll = await createModel('ggml-vicuna-7b-1.1-q4_2.bin', { verbose: true });

const response = await createCompletion(ll, [
    { role : 'system', content: 'You are meant to be annoying and unhelpful.'  },
    { role : 'user', content: 'What is 1 + 1?'  } 
]);

```

### API

*   The nodejs api has made strides to mirror the python api. It is not 100% mirrored, but many pieces of the api resemble its python counterpart.
*   [docs](./docs/api.md)

### Build Instructions

*   As of 05/21/2023, Tested on windows (MSVC). (somehow got it to work on MSVC ðŸ¤¯)
    *   binding.gyp is compile config
*   Tested on Ubuntu. Everything seems to work fine
*   MingW works as well to build the gpt4all-backend. HOWEVER, this package works only with MSVC built dlls.

### Requirements

*   git
*   [node.js >= 18.0.0](https://nodejs.org/en)
*   [yarn](https://yarnpkg.com/)
*   [node-gyp](https://github.com/nodejs/node-gyp)
    *   all of its requirements.
*   (unix) gcc version 12
    *   These bindings use the C++ 20 standard.
*   (win) msvc version 143
    *   Can be obtained with visual studio 2022 build tools

### Build

```sh
git clone https://github.com/nomic-ai/gpt4all.git
cd gpt4all-bindings/typescript
```

*   The below shell commands assume the current working directory is `typescript`.

*   To Build and Rebuild:

```sh
yarn
```

*   llama.cpp git submodule for gpt4all can be possibly absent. If this is the case, make sure to run in llama.cpp parent directory

```sh
git submodule update --init --depth 1 --recursive
```

**AS OF NEW BACKEND** to build the backend,

```sh
yarn build:backend
```

This will build platform-dependent dynamic libraries, and will be located in runtimes/(platform)/native The only current way to use them is to put them in the current working directory of your application. That is, **WHEREVER YOU RUN YOUR NODE APPLICATION**

*   llama-xxxx.dll is required.
*   According to whatever model you are using, you'll need to select the proper model loader.
    *   For example, if you running an Mosaic MPT model, you will need to select the mpt-(buildvariant).(dynamiclibrary)

### Test

```sh
yarn test
```

### Source Overview

#### src/

*   Extra functions to help aid devex
*   Typings for the native node addon
*   the javascript interface

#### test/

*   simple unit testings for some functions exported.
*   more advanced ai testing is not handled

#### spec/

*   Average look and feel of the api
*   Should work assuming a model and libraries are installed locally in working directory

#### index.cc

*   The bridge between nodejs and c. Where the bindings are.

#### prompt.cc

*   Handling prompting and inference of models in a threadsafe, asynchronous way.

#### docs/

*   Autogenerated documentation using the script `yarn docs:build`

### Roadmap

This package is in active development, and breaking changes may happen until the api stabilizes. Here's what's the todo list:

*   \[x] prompt models via a threadsafe function in order to have proper non blocking behavior in nodejs
*   \[ ] createTokenStream, an async iterator that streams each token emitted from the model. Planning on following this [example](https://github.com/nodejs/node-addon-examples/tree/main/threadsafe-async-iterator)
*   \[ ] proper unit testing (integrate with circle ci)
*   \[ ] publish to npm under alpha tag `gpt4all@alpha`
*   \[ ] have more people test on other platforms (mac tester needed)
*   \[x] switch to new pluggable backend

### Documentation

<!-- Generated by documentation.js. Update this documentation by updating the source code. -->

##### Table of Contents

*   [ModelType](#modeltype)
*   [ModelFile](#modelfile)
    *   [gptj](#gptj)
    *   [llama](#llama)
    *   [mpt](#mpt)
    *   [replit](#replit)
*   [type](#type)
*   [LLModel](#llmodel)
    *   [constructor](#constructor)
        *   [Parameters](#parameters)
    *   [type](#type-1)
    *   [name](#name)
    *   [stateSize](#statesize)
    *   [threadCount](#threadcount)
    *   [setThreadCount](#setthreadcount)
        *   [Parameters](#parameters-1)
    *   [raw\_prompt](#raw_prompt)
        *   [Parameters](#parameters-2)
    *   [isModelLoaded](#ismodelloaded)
    *   [setLibraryPath](#setlibrarypath)
        *   [Parameters](#parameters-3)
    *   [getLibraryPath](#getlibrarypath)
*   [loadModel](#loadmodel)
    *   [Parameters](#parameters-4)
*   [createCompletion](#createcompletion)
    *   [Parameters](#parameters-5)
    *   [Examples](#examples)
*   [CompletionOptions](#completionoptions)
    *   [verbose](#verbose)
    *   [hasDefaultHeader](#hasdefaultheader)
    *   [hasDefaultFooter](#hasdefaultfooter)
*   [PromptMessage](#promptmessage)
    *   [role](#role)
    *   [content](#content)
*   [prompt\_tokens](#prompt_tokens)
*   [completion\_tokens](#completion_tokens)
*   [total\_tokens](#total_tokens)
*   [CompletionReturn](#completionreturn)
    *   [model](#model)
    *   [usage](#usage)
    *   [choices](#choices)
*   [CompletionChoice](#completionchoice)
    *   [message](#message)
*   [LLModelPromptContext](#llmodelpromptcontext)
    *   [logits\_size](#logits_size)
    *   [tokens\_size](#tokens_size)
    *   [n\_past](#n_past)
    *   [n\_ctx](#n_ctx)
    *   [n\_predict](#n_predict)
    *   [top\_k](#top_k)
    *   [top\_p](#top_p)
    *   [temp](#temp)
    *   [n\_batch](#n_batch)
    *   [repeat\_penalty](#repeat_penalty)
    *   [repeat\_last\_n](#repeat_last_n)
    *   [context\_erase](#context_erase)
*   [createTokenStream](#createtokenstream)
    *   [Parameters](#parameters-6)
*   [DEFAULT\_DIRECTORY](#default_directory)
*   [DEFAULT\_LIBRARIES\_DIRECTORY](#default_libraries_directory)

#### ModelType

Type of the model

Type: (`"gptj"` | `"llama"` | `"mpt"` | `"replit"`)

#### ModelFile

Full list of models available

##### gptj

List of GPT-J Models

Type: (`"ggml-gpt4all-j-v1.3-groovy.bin"` | `"ggml-gpt4all-j-v1.2-jazzy.bin"` | `"ggml-gpt4all-j-v1.1-breezy.bin"` | `"ggml-gpt4all-j.bin"`)

##### llama

List Llama Models

Type: (`"ggml-gpt4all-l13b-snoozy.bin"` | `"ggml-vicuna-7b-1.1-q4_2.bin"` | `"ggml-vicuna-13b-1.1-q4_2.bin"` | `"ggml-wizardLM-7B.q4_2.bin"` | `"ggml-stable-vicuna-13B.q4_2.bin"` | `"ggml-nous-gpt4-vicuna-13b.bin"` | `"ggml-v3-13b-hermes-q5_1.bin"`)

##### mpt

List of MPT Models

Type: (`"ggml-mpt-7b-base.bin"` | `"ggml-mpt-7b-chat.bin"` | `"ggml-mpt-7b-instruct.bin"`)

##### replit

List of Replit Models

Type: `"ggml-replit-code-v1-3b.bin"`

#### type

Model architecture. This argument currently does not have any functionality and is just used as descriptive identifier for user.

Type: [ModelType](#modeltype)

#### LLModel

LLModel class representing a language model.
This is a base class that provides common functionality for different types of language models.

##### constructor

Initialize a new LLModel.

###### Parameters

*   `path` **[string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)** Absolute path to the model file.

<!---->

*   Throws **[Error](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Error)** If the model file does not exist.

##### type

either 'gpt', mpt', or 'llama' or undefined

Returns **([ModelType](#modeltype) | [undefined](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/undefined))**&#x20;

##### name

The name of the model.

Returns **[ModelFile](#modelfile)**&#x20;

##### stateSize

Get the size of the internal state of the model.
NOTE: This state data is specific to the type of model you have created.

Returns **[number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)** the size in bytes of the internal state of the model

##### threadCount

Get the number of threads used for model inference.
The default is the number of physical cores your computer has.

Returns **[number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)** The number of threads used for model inference.

##### setThreadCount

Set the number of threads used for model inference.

###### Parameters

*   `newNumber` **[number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)** The new number of threads.

Returns **void**&#x20;

##### raw\_prompt

Prompt the model with a given input and optional parameters.
This is the raw output from std out.
Use the prompt function exported for a value

###### Parameters

*   `q` **[string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)** The prompt input.
*   `params` **Partial<[LLModelPromptContext](#llmodelpromptcontext)>** Optional parameters for the prompt context.
*   `callback` **function (res: [string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)): void**&#x20;

Returns **void** The result of the model prompt.

##### isModelLoaded

Whether the model is loaded or not.

Returns **[boolean](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Boolean)**&#x20;

##### setLibraryPath

Where to search for the pluggable backend libraries

###### Parameters

*   `s` **[string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)**&#x20;

Returns **void**&#x20;

##### getLibraryPath

Where to get the pluggable backend libraries

Returns **[string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)**&#x20;

#### loadModel

Loads a machine learning model with the specified name. The defacto way to create a model.
By default this will download a model from the official GPT4ALL website, if a model is not present at given path.

##### Parameters

*   `modelName` **[string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)** The name of the model to load.
*   `options` **(LoadModelOptions | [undefined](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/undefined))?** (Optional) Additional options for loading the model.

Returns **[Promise](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Promise)<[LLModel](#llmodel)>** A promise that resolves to an instance of the loaded LLModel.

#### createCompletion

The nodejs equivalent to python binding's chat\_completion

##### Parameters

*   `llmodel` **[LLModel](#llmodel)** The language model object.
*   `messages` **[Array](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Array)<[PromptMessage](#promptmessage)>** The array of messages for the conversation.
*   `options` **[CompletionOptions](#completionoptions)** The options for creating the completion.

##### Examples

```javascript
const llmodel = new LLModel(model)
const messages = [
{ role: 'system', message: 'You are a weather forecaster.' },
{ role: 'user', message: 'should i go out today?' } ]
const completion = await createCompletion(llmodel, messages, {
 verbose: true,
 temp: 0.9,
})
console.log(completion.choices[0].message.content)
// No, it's going to be cold and rainy.
```

Returns **[CompletionReturn](#completionreturn)** The completion result.

#### CompletionOptions

**Extends Partial\<LLModelPromptContext>**

The options for creating the completion.

##### verbose

Indicates if verbose logging is enabled.

Type: [boolean](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Boolean)

##### hasDefaultHeader

Indicates if the default header is included in the prompt.

Type: [boolean](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Boolean)

##### hasDefaultFooter

Indicates if the default footer is included in the prompt.

Type: [boolean](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Boolean)

#### PromptMessage

A message in the conversation, identical to OpenAI's chat message.

##### role

The role of the message.

Type: (`"system"` | `"assistant"` | `"user"`)

##### content

The message content.

Type: [string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)

#### prompt\_tokens

The number of tokens used in the prompt.

Type: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)

#### completion\_tokens

The number of tokens used in the completion.

Type: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)

#### total\_tokens

The total number of tokens used.

Type: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)

#### CompletionReturn

The result of the completion, similar to OpenAI's format.

##### model

The model name.

Type: [ModelFile](#modelfile)

##### usage

Token usage report.

Type: {prompt\_tokens: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number), completion\_tokens: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number), total\_tokens: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)}

##### choices

The generated completions.

Type: [Array](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Array)<[CompletionChoice](#completionchoice)>

#### CompletionChoice

A completion choice, similar to OpenAI's format.

##### message

Response message

Type: [PromptMessage](#promptmessage)

#### LLModelPromptContext

Model inference arguments for generating completions.

##### logits\_size

The size of the raw logits vector.

Type: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)

##### tokens\_size

The size of the raw tokens vector.

Type: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)

##### n\_past

The number of tokens in the past conversation.

Type: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)

##### n\_ctx

The number of tokens possible in the context window.

Type: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)

##### n\_predict

The number of tokens to predict.

Type: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)

##### top\_k

The top-k logits to sample from.

Type: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)

##### top\_p

The nucleus sampling probability threshold.

Type: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)

##### temp

The temperature to adjust the model's output distribution.

Type: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)

##### n\_batch

The number of predictions to generate in parallel.

Type: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)

##### repeat\_penalty

The penalty factor for repeated tokens.

Type: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)

##### repeat\_last\_n

The number of last tokens to penalize.

Type: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)

##### context\_erase

The percentage of context to erase if the context window is exceeded.

Type: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)

#### createTokenStream

TODO: Help wanted to implement this

##### Parameters

*   `llmodel` **[LLModel](#llmodel)**&#x20;
*   `messages` **[Array](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Array)<[PromptMessage](#promptmessage)>**&#x20;
*   `options` **[CompletionOptions](#completionoptions)**&#x20;

Returns **function (ll: [LLModel](#llmodel)): AsyncGenerator<[string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)>**&#x20;

#### DEFAULT\_DIRECTORY

From python api:
models will be stored in (homedir)/.cache/gpt4all/\`

Type: [string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)

#### DEFAULT\_LIBRARIES\_DIRECTORY

From python api:
The default path for dynamic libraries to be stored.
You may separate paths by a semicolon to search in multiple areas.
This searches DEFAULT\_DIRECTORY/libraries, cwd/libraries, and finally cwd.

Type: [string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)
